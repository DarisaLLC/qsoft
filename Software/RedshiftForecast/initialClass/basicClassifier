####
#### apply CART and RF classifiers with cost function / prior
####

#### by James Long
#### June 3, 2010

# load the packages
library('foreign')
library('rpart')
library('tree')
library('randomForest')
library('xtable')










###
### good code for running functions file below
###


# load functions we will use
source('functions')


# setup parameters, create output matrix
CUTOFF = 4
COST = c(3,5,7)
outTable = matrix(0,nrow=length(COST)*2,ncol=3)
outTable = data.frame(outTable)
names(outTable) = c("","Without Error Features","With Error Features")

outRowNames = rep("",length(COST)*2)
for(i in 1:length(COST)){
	outRowNames[2*i-1] = paste("Cost",COST[i])
}
outTable[,1] = outRowNames


# load the data
data1 = read.arff('redshiftmachine.arff')

# remove the error features from the data
data1 = removeErrors(data1)

##
## run without error features

figureName = c("GRB Tree: High > 4, Cost = 3 w/o Error Feat.","GRB Tree: High > 4, Cost = 5 w/o Error Feat.","GRB Tree: High > 4, Cost = 7 w/o Error Feat.")
figureFile = c("noErrorCost3.pdf","noErrorCost5.pdf","noErrorCost7.pdf")

for(i in 1:length(COST)){
	thetree = grbTree(data1,CUTOFF,COST[[i]],figureFile[i],figureName[i])
	loss = sum(diag(thetree[[3]][[2]])* c(1,COST[[i]]))
	loss = round(loss / sum(colSums(thetree[[3]][[2]]) * c(1, COST[[i]])),3)
	highaslow = round(data1$Z[thetree[[3]][[1]]],1)
	highaslow = paste(highaslow,sep=',',collapse=', ')
	highaslow = paste("(",highaslow,")",sep="")
	outTable[2*i - 1,2] = loss
	outTable[2*i,2] = highaslow
}









# load the data, this time keep error features
data1 = read.arff('redshiftmachine.arff')

##
## run with error features

figureName = c("GRB Tree: High > 4, Cost = 3 with Error Feat.","GRB Tree: High > 4, Cost = 5 with Error Feat.","GRB Tree: High > 4, Cost = 7 with Error Feat.")
figureFile = c("ErrorCost3.pdf","ErrorCost5.pdf","ErrorCost7.pdf")

for(i in 1:length(COST)){
	thetree = grbTree(data1,CUTOFF,COST[[i]],figureFile[i],figureName[i])
	loss = sum(diag(thetree[[3]][[2]])* c(1,COST[[i]]))
	loss = round(loss / sum(colSums(thetree[[3]][[2]]) * c(1, COST[[i]])),3)
	highaslow = round(data1$Z[thetree[[3]][[1]]],1)
	highaslow = paste(highaslow,sep=',',collapse=', ')
	highaslow = paste("(",highaslow,")",sep="")
	outTable[2*i - 1,3] = loss
	outTable[2*i,3] = highaslow
}







outTable





outputX = xtable(outTable,caption="Performance of CART for Two Sets of Features and 3 Loss Functions",label="tab:errors",align=rep('c',length(outTable)+1))
print(outputX,type='latex',file='results.tex',table.placement="H",include.rownames=F,append=F,hline.after=c(0,2*(1:(nrow(outTable)/2))))








####
#### random forest classifier
####


data1 = na.roughfix(data1)      
fit.rf = randomForest(Z ~ ., importance=TRUE,classwt=c(1,1000),ntree=100,data=data1)


fit.rf





