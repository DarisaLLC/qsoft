names(data1) 
names(data1)
data1$Z
sum(data1$Z > 4)
names(data1)
names(data1) 
data1$class
table(data1$class)
names(data1)
names(data1) 
names(data1$uvot_detection)
table(data1$uvot_detection)
sum(data1$uvot_detection & data1$class == 'high')
sum(data1$uvot_detection == 'no' & data1$class == 'high')
sum(data1$uvot_detection == 'no')
sum(data1$uvot_detection == 'no' & data1$class == 'high')
sum(data1$uvot_detection == 'yes' & data1$class == 'high')
names(data1)
data1
head(data1)
names(data1)
head(data1)
'number_or_useless_features' == 'number_of_useless_features'
names(useless_data)
paste('V',1:10)
paste('V',1:10,sep="")
colnames(useless_features)
colnames(useless_data)
names(data1)
fit1
fit2
fit3
prior_alpha1
prior_alpha3
prior_alpha5
fit1 
fit1$cptable
fit
fit1
fit2$cptable
fit2
fit3
fit3$cptable
fit3
prior_alpha1
fit1
fit1$cptable
plot(fit1)
text(fit1)
plot(fit3)
text(fit3)
fit1$cptable
fit3$cptable
fit1$cptable
prior_alpha1
fit1$cptable[,3]
> 'hello'
'hello'
warnings()
warnings()
fit1
prior_alpha1[11]
fit1[1]
i
  fit1[i] = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data = data1)
 rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data = data1)
 rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data = data1)
 rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class"
 rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data=data1)
 rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data=data1)
 rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data=data1)
 fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data=data1)
fit1 = list()
class(fit1)
fit1[1]
 fit1[1] = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data=data1)
fit1
 fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data=data1)
fit1
summary(fit1)
names(fit1) 
print(fit1)
warnings
warnings()
fit1[1]
fit2
n
names(fit2)
fit2$csplit
fit2$splits
names(fit2)
fit2$frame
names(fit2)
'
'
fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data = data1)
fit1 = fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data = data1)
fit1 = fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data = data1)
fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data = data1)
fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[12],prior_alpha1[12])),method="class",data = data1)
rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[i],prior_alpha1[i])),method="class",data = data1)
fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[12],prior_alpha1[12])),method="class",data = data1)
prior_alpha[12]
prior_alpha1[12]
fit1$cptable
fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[8],prior_alpha1[8])),method="class",data = data1)
prior_alpha1[8]
fit1$cptable
fit1 = rpart(class ~ .,parms=list(prior=c(1-prior_alpha1[6],prior_alpha1[6])),method="class",data = data1)
prior_alpha1[6]
fit1$cptable
q()
y
trees[[1]]
trees[[2]]
trees[[3]]
length(trees)
print('hello' + 3)
print(paste('hello',3))
paste('prior on being high:', i / 20)
table(data1$uvot_detection,data1$high)
names(data1)
table(data1$uvot_detection,data1$class)
q()
y
ls()
fit1
fit3
fit3[[1]]
fit3[[2]]
q()
y
results
results 
.99^20
.99^40
()
q()
y
########
########
######## Get GRB data in form so we can run through algorithm1.R
########
######## by James Long
######## date Nov 16, 2010
########
# necessary packages
set.seed(250)
library('foreign')
library('rpart')
library('plotrix')
library('xtable')
source('algorithm1.R')
##
## get the data in the desired form
##
#filename = '070710_shortremoved_NoZremoved.arff_full'
filename='uvot_no_error.arff'
#filename = '070710_shortremoved_NoZremoved_outremoved_late_proccessed.arff'
data1 = read.arff(filename)
Z = data1$Z
data1$triggerid_str = NULL # meaningless feature in some of the data sets
data1 = removeErrors(data1) # get rid of error features
data1 = cleanData(data1,4) # define above 4 as high, below 4 as low
# these features are removed because they are formatted weirdly, according to Adam they
# probably are not important in prediction anyway
data1 = subset(data1,select=(!(names(data1) %in% c("CHI2_PC","CHI2_WT","CHI2_PC_LATE"))))
# uncomment the following line if want code to run fast (vastly reduced # of features)
# this is used mostly for testing
# data1 = subset(data1,select=((names(data1) %in% c("class","A","FLX_PC_LATE","wh_mag_isupper"))))
# only use uvot_detection feature
data1 = subset(data1,select=((names(data1) %in% c("class","uvot_detection"))))
# features names Adam thinks are important are stored in vector good_features
# recall there are 4 sets of features we get, say F1,F2,F3, and F4 where F1<F2<F3<F4
# (we get more features as time passes). if we are analyzing feature
# set F3 we can analyze the features that are in F3 and ''good_features''
#good_features = c("class","A","B","EP0","FL","FLX_PC_LATE","GAM_PC","MAX_SNR","NH_PC","T90","bat_image_signif","bat_image_peak","bat_trigger_dur","bat_is_rate_trig","v_mag_isupper")
# uncomment line below if you only want to use Adam's recommended feature set
#data1 = subset(data1,select=((names(data1) %in% good_features)))
###
### Important PLEASE READ: data1 should now be dataframe with first column ''class'' a factor with two 
### levels. Remaining columns are features, which may be continuous and/or categorical, 
### missingness okay. The actual redshift (numerical value) should be stored in the variable
### Z. The order of Z should match the observation order in data1.
###
###
### some info about this run for the user to see
###
print('number of features being used:')
print(length(data1) - 1)
print('number of observations:')
print(nrow(data1))
print('number of high grbs:')
print(sum(data1$class=="high"))
print('number of low grbs:')
print(sum(data1$class=="low"))
print('names of features:')
print(names(data1))
table(data1$class,data1$uvot_detection)
names(data1)
number_of_useless_features = 1
useless_data = matrix(runif(nrow(data1)*number_of_useless_features),nrow=nrow(data1))
colnames(useless_data) = paste('f',1:number_of_useless_features,sep="")
data1 = cbind(data1,useless_data)
names(data1)
implement(data1,Z,HEAT_MAP=FALSE)
########
########
######## Get GRB data in form so we can run through algorithm1.R
########
######## by James Long
######## date Nov 16, 2010
########
# necessary packages
set.seed(250)
library('foreign')
library('rpart')
library('plotrix')
library('xtable')
source('algorithm1.R')
##
## get the data in the desired form
##
#filename = '070710_shortremoved_NoZremoved.arff_full'
filename='uvot_no_error.arff'
#filename = '070710_shortremoved_NoZremoved_outremoved_late_proccessed.arff'
data1 = read.arff(filename)
Z = data1$Z
data1$triggerid_str = NULL # meaningless feature in some of the data sets
data1 = removeErrors(data1) # get rid of error features
data1 = cleanData(data1,4) # define above 4 as high, below 4 as low
# these features are removed because they are formatted weirdly, according to Adam they
# probably are not important in prediction anyway
data1 = subset(data1,select=(!(names(data1) %in% c("CHI2_PC","CHI2_WT","CHI2_PC_LATE"))))
# uncomment the following line if want code to run fast (vastly reduced # of features)
# this is used mostly for testing
# data1 = subset(data1,select=((names(data1) %in% c("class","A","FLX_PC_LATE","wh_mag_isupper"))))
# only use uvot_detection feature
data1 = subset(data1,select=((names(data1) %in% c("class","uvot_detection"))))
# features names Adam thinks are important are stored in vector good_features
# recall there are 4 sets of features we get, say F1,F2,F3, and F4 where F1<F2<F3<F4
# (we get more features as time passes). if we are analyzing feature
# set F3 we can analyze the features that are in F3 and ''good_features''
#good_features = c("class","A","B","EP0","FL","FLX_PC_LATE","GAM_PC","MAX_SNR","NH_PC","T90","bat_image_signif","bat_image_peak","bat_trigger_dur","bat_is_rate_trig","v_mag_isupper")
# uncomment line below if you only want to use Adam's recommended feature set
#data1 = subset(data1,select=((names(data1) %in% good_features)))
###
### Important PLEASE READ: data1 should now be dataframe with first column ''class'' a factor with two 
### levels. Remaining columns are features, which may be continuous and/or categorical, 
### missingness okay. The actual redshift (numerical value) should be stored in the variable
### Z. The order of Z should match the observation order in data1.
###
###
### some info about this run for the user to see
###
print('number of features being used:')
print(length(data1) - 1)
print('number of observations:')
print(nrow(data1))
print('number of high grbs:')
print(sum(data1$class=="high"))
print('number of low grbs:')
print(sum(data1$class=="low"))
print('names of features:')
print(names(data1))
###
### add some useless features and rerun algorithm
###
names(data1)
number_of_useless_features = 10
useless_data = matrix(runif(nrow(data1)*number_of_useless_features),nrow=nrow(data1))
colnames(useless_data) = paste('f',1:number_of_useless_features,sep="")
data1 = cbind(data1,useless_data)
names(data1)
implement(data1,Z,HEAT_MAP=FALSE)
q()
