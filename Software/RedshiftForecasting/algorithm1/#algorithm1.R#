#######
####### Classification of Gamma Ray Bursts
####### date: Nov 14, 2010
#######

###### Note 1: This file contains functions for implementing algorithm 1. Code is
###### highly commented.

###### Note 2: Even quite recently the function outputSplits was producing incorrect
###### results. The purpose of output splits is to let the user know what features
###### CART was splitting on. Also the output correct.tex was (ironically) often
###### incorrect. correct.tex is suppose to tell the user which GRBs were classified
###### correctly as a way to examine if high bursts were classified right / how often.
###### These may have been fixed but results should be examined closely for
###### signs of error. The other aspects of the program are more stable, but this is
###### a relative term.







########
########
######## SECTION 1: FUNCTIONS
########
########
########

library('rpart')
library('plotrix')
library('xtable')


# get rid of error features in data, up to user if you want to do this.
# code is written for Adam Morgan's GRB data set where the error features
# have names some_feature_name_negerr and some_feature_name_poserr
removeErrors = function(data1){
	neg = grep("*_negerr", names(data1))
	pos = grep("*_poserr", names(data1))
	errors = c(neg,pos)
	errors = errors[order(errors)]
	errors = 1:length(data1) %in% errors
	data1 = subset(data1,select = !errors)	
	return(data1)
}

# generates k equally sized disjoint sets containing the numbers 1 through len.
# returns as list, each element in list is vector of numbers
# used to randomly select folds in the cross validation steps
findFolds = function(len,k){
	sets = list()
	indices = 1:len
	base = floor( len / k )
	left = len %% k 
	if(left > 0){
		for( i in 1:left ){
			sets[[i]] = sample(indices,base + 1)
			indices = indices[!(indices %in% sets[[i]])]
		}
	}
	for( i in (left + 1):k ){
		sets[[i]] = sample(indices,base)
		indices = indices[!(indices %in% sets[[i]])]	
	}
	return(sets)
}



# used to get the astro data to look right, converts continuous redshift Z into
# high / low factor class, high is greater than Zcutoff
cleanData = function(data1,Zcutoff){
	classZ = factor(rep("low",nrow(data1)),levels=c("low","high"))
	classZ[data1$Z > Zcutoff] = "high"
	data1$Z = classZ
	names(data1) = c("class",names(data1)[2:length(data1)])
	return(data1)
}



##
## implementation of Tamara's "algorithm 1" (see pseudocode in email)
## 1. data1 is the data
## 2. nfolds is scalar, the number of folds for cross validation (usually 10)
## 3. priorHigh is vector of priors. we want to choose the prior that when used in
##      CART will give the best performance
## 4. alpha is the maximum proportion of low that can classify as high, 50% is reasonable
##
## this function returns the prior (in priorHigh vector) that maximizes high as
## high after the confusion matrices have been forced to fill the alpha condition
##
algorithm1 = function(data1,nfolds,priorHigh,alpha){
	folds = findFolds(nrow(data1),nfolds) # get the folds for CV
	confusionMatrices = array(0,c(length(priorHigh),2,2)) # for holding results
	for(i in 1:nfolds){
  		confusionMatrices = confusionMatrices + confusionCV(data1,folds[[i]],priorHigh)
	}
	# using confusionMatrices, find best prior
	c = choosePrior(confusionMatrices,alpha) # determine index of best prior, 0 if no priors
    c = priorHigh[c] # determine the actual prior, not the index
	return(c)
}

##
## used by function algorithm1, see comments there
##
confusionCV = function(data1,leftOut,priorHigh){
	train = !( (1:nrow(data1)) %in% leftOut ) # convert to boolean
	test = (1:nrow(data1)) %in% leftOut # convert to boolean
	data1train = subset(data1,subset=train) # separate training and test sets
	data1test = subset(data1,subset=test) # separate training and test sets
	confusion = array(0,c(length(priorHigh),2,2)) # for holding confusion matrices
	for(i in 1:length(priorHigh)){
		  fit1 = rpart(class ~ .,parms=list(prior=c(1-priorHigh[i],priorHigh[i])),method="class",data=data1train)
          # prune the tree, this is some question as to whether this is the optimal pruning
          # stategy, already investigated this a bit but might want to do more
 		  #if(length(fit1$cptable) > 3){
          #  bestRow = which.min(fit1$cptable[,4])
		#	cp = fit1$cptable[bestRow,1]
		 # 	fit1 = prune(fit1,cp=cp)
		 # }
          # make predictions on the test data
		  predictions = predict(fit1,type='class',newdata=data1test)

          # some logical vectors so we can fill out the confusion matrix
		  predictionsHigh = predictions == "high"
		  actualHigh = data1test$class == "high"

          # fill out confusion matrices
		  confusion[i,,] = matrix(c(sum(!predictionsHigh & !actualHigh),sum(predictionsHigh & !actualHigh),sum(!predictionsHigh & actualHigh),sum(predictionsHigh & actualHigh)),nrow=2)
	}
	return(confusion)
}


# forces confusionMatrix to satisfy alpha condition exactly
# this code is horrendously inefficient, a better way?
#
# 1. confusionMatrix is a 2 x 2 confusion matrix
# 2. alpha is the proportion of obs we can classify as high (set by astronomers)
#
# we output a new matrix that exactly satisfies alpha condition by randomly assigning
# observations to low or high depending on whether the argument confusionMatrix is
# over the alpha condition or under the alpha condition
fillAlpha = function(confusionMatrix,alpha){
	confusionOut = matrix(0,nrow=2,ncol=2)
	totalObs = sum(confusionMatrix)
	asHigh = confusionMatrix[2,1] + confusionMatrix[2,2]
	change = totalObs*alpha - asHigh
    # totalObs*alpha is the number we can classify as high so 'change' is the
    # number of bursts we can shift to high and still be under the threshold
    # (note if change is negative it is the number of bursts we must shift to
    # low in order to be within threshold)
	c = change / (totalObs - asHigh)
    # shifting lows to high, we did not fill up threshold = totalObs*alpha
	if(c > 0){
		 confusionOut[2,1] = c*confusionMatrix[1,1] + confusionMatrix[2,1]
		 confusionOut[2,2] = c*confusionMatrix[1,2] + confusionMatrix[2,2]
		 confusionOut[1,1] = (1-c)*confusionMatrix[1,1]
		 confusionOut[1,2] = (1-c)*confusionMatrix[1,2]
	}
    # shift highs to low, we went over and now have to assign some highs as low
	if(c < 0){
		 c = -1 * change / asHigh
		 confusionOut[1,1] = c*confusionMatrix[2,1] + confusionMatrix[1,1]
		 confusionOut[1,2] = c*confusionMatrix[2,2] + confusionMatrix[1,2]
		 confusionOut[2,1] = (1-c)*confusionMatrix[2,1]
		 confusionOut[2,2] = (1-c)*confusionMatrix[2,2]
	}
    # we hit it exactly right, no changes need to be made
	if(c == 0){
		 confusionOut = confusionMatrix
	}
	return(confusionOut)
}


##
## accepts a set of confusion matrices and returns the index of the matrix with 
## most high as high. before comparing high as high this function forces each matrix to
## satify the alpha condition (for loop at the beginning of the function - fillAlpha).
## The fillAlpha function ensures that each matrix classifies exactly alpha proportion
## of bursts as high (see comments of this function)
##
choosePrior = function(confusionMatrices,alpha){
	for(i in 1:length(confusionMatrices[,2,2])){
		  confusionMatrices[i,,] = fillAlpha(confusionMatrices[i,,],alpha)
	}
	bestModel = which.max(confusionMatrices[,2,2])
	return(bestModel)
}


##
## runs algorithm 1 nalgorithm1 times and returns the prior on high that algorithm1
## chooses the most. we decided on using this function after it was found that
## algorithm 1 sometimes chooses different priors because of randomness in CV. we
## could cut this out of our method (for speed or theoretical reasons) and just
## run algorithm1 once and use the first prior it chooses
##
algorithm1CV = function(data1,nfolds,priorHigh,alpha,nalgorithm1){
	choices = matrix(0,nrow=(length(priorHigh)+1),ncol=2)
	choices[,1] = c(0,priorHigh)
	for(i in 1:nalgorithm1){
		  choice = algorithm1(data1,nfolds,priorHigh,alpha)
		  choices[choices[,1] == choice,2] = choices[choices[,1] == choice,2] + 1
	}
	bestC = which.max(choices[,2])
	return(choices[bestC,1])
}



##
## cross validate to determine how well algorithm 1 is doing
##
## 1. data1 is the data
## 2. priorHigh is a vector of prior on a GRB being high
## 3. alpha is a proportion, an upper bound on the proportion of low classified as high
## 4. nfolds1 is the number of folds in the outer CV step, initiated in this loop
## 5. nfolds2 is the number of fold in inner CV step, initiated by algorithm1
##
crossValidateAlgorithm = function(data1,priorHigh,alpha,nfolds1,nfolds2,nalgorithm1,Z){
	folds = findFolds(nrow(data1),nfolds1)
	confusion = matrix(0,nrow=2,ncol=2)
	correct = rep(0,nrow(data1))
	theFits = list()
	for(i in 1:nfolds1){
		  train = !( (1:nrow(data1)) %in% folds[[i]] ) # convert to boolean
		  test = (1:nrow(data1)) %in% folds[[i]] # convert to boolean
		  data1train = subset(data1,subset=train) # separate training and test sets
		  data1test = subset(data1,subset=test) # separate training and test sets
		  bestPrior = algorithm1CV(data1train,nfolds2,priorHigh,alpha,nalgorithm1)
		  if(bestPrior != 0){
		  	 fit1 = rpart(class ~ .,parms=list(prior=c(1-bestPrior,bestPrior)),method="class",data=data1train)
			 #if(length(fit1$cptable) > 3){
		  	#	bestRow = which.min(fit1$cptable[,4])
			#	cp = fit1$cptable[bestRow,1]
		  	#	fit1 = prune(fit1,cp=cp)
			 #}
			 theFits[[i]] = fit1
			 predictions = predict(fit1,type='class',newdata=data1test) # predict test set
		  }
		  if(bestPrior == 0){
		  	 theFits[[i]] = 0
		  	 predictions = rep("low",nrow(data1test)) # classify as low if no priors work
		  }

		  predictionsHigh = predictions == "high"
		  actualHigh = data1test$class == "high"

		  confusion = confusion + matrix(c(sum(!predictionsHigh & !actualHigh),sum(predictionsHigh & !actualHigh),sum(!predictionsHigh & actualHigh),sum(predictionsHigh & actualHigh)),nrow=2)
		  correct[test] = 1*(actualHigh == predictionsHigh)
	}
    # confusion tells us how well algorithm1 did, theFits output the trees so later
    # we can determine which variables were split on. correct tells us which observations
    # were classified correctly, this is used because we want to make sure we are
    # classifying high bursts correctly
	return(list(confusion,theFits,correct))
}




# accepts
# 1. fits = a list of trees
# 2. alpha = the alpha for which these trees were calculated
# 3. filename = the file to put the pretty table in
# 4. number = the number of times this function has been called, on the
#   first call the file ''filename'' is created, on subsequent calls we
#   append to it
#
# the purporse of this function is to give an idea of what features the tree
# are using as splits and thus measure feature importance in a rough manner
#
outputSplits = function(fits,alpha,filename1,number){
	# turn lists fits into table of number splits
	importance = data.frame( names(data1)[2:length(data1)] , 0 )
	names(importance) = c("Feature","Number Times")
	totalTrees = 0
	for(i in 1:length(fits)){
		  for(j in 1:length(fits[[i]])){
		  		totalTrees = totalTrees + 1
		  		if(class(fits[[i]][[j]]) == "rpart"){
		      		varsUsed = (fits[[i]][[j]])$frame[,1]
		  	  		varsUsed = varsUsed[varsUsed!="<leaf>"]
			  		importance[,2] = importance[,2] + (importance$Feature %in% varsUsed)
				}
		  }
	}
	# get variable importance looking nice
	importance = importance[order(importance[,2],decreasing=T),]
	importance = subset(importance,subset=(importance[,2] > 0))
	if(nrow(importance) == 0){
		importance[,1] = as.character(importance[,1])
		importance[1,] = c("No Features Used",0)
	}

	# output the importance plot to a file filename1
	if(number==1){
		append1 = FALSE
	}
	if(number!=1){
		append1 = TRUE
	}
	caption1 = paste("Splits for alpha =",alpha,"model. The Number Times column is the number of trees the given feature appeared in out of a total of",totalTrees,"created for cross validation.")
	outputX = xtable(importance,caption=caption1,align=c('c','c','c'),digits=0)
	print(outputX,type='latex',append=append1,file=filename1,table.placement="H",include.rownames=F)
}


## make the confusion matrices look nice for output in .tex, added quantiles to values,
## ect.
prettyConfusion	= function(confusion,alpha,number,filename1){
	prettyOut = matrix(rep("Empty",4),nrow=2)
	for(i in 1:2){
		for(j in 1:2){
			x = confusion[i,j,]
			mean1 = round(mean(x),2)
			q1 = round(quantile(x,.25),2)
			q2 = round(quantile(x,.75),2)
			prettyOut[i,j] = paste(mean1,"(",q1,",",q2,")",collapse="")
		}
	}
	rownames(prettyOut) = c("Pred. Low","Pred High")
	colnames(prettyOut) = c("Actual Low","Actual High")
	caption1 = paste("CV Confusion Matrix for alpha =",alpha,". The parentheses are .25 and .75 quantiles.")	
	if(number==1){
		append1 = FALSE
	}
	if(number!=1){
		append1 = TRUE
	}
	outputX = xtable(prettyOut,caption=caption1)
	print(outputX,type='latex',filename1,table.placement="H",include.rownames=T,append=append1)
}




####
#### heat map function:
####
####
#### here we assess how stable algorithm 1's selection of optimal prior is, this produces
#### a heat map but does not have to be run to get results
####
heatMap = function(data1,ALPHA_RANGE = 1:9 / 10,PRIOR_RANGE = (1:19) / 20,NUMBER=10){

  toPlot = matrix(0,nrow=length(PRIOR_RANGE)+1,ncol=length(ALPHA_RANGE))
  colnames(toPlot) = ALPHA_RANGE
  rownames(toPlot) = c(PRIOR_RANGE[order(PRIOR_RANGE,decreasing=T)],0)

  print("computing heat map")
  for(i in 1:length(ALPHA_RANGE)){
    print(paste("iteration:",i,"of",length(ALPHA_RANGE)))
    for(j in 1:NUMBER){
            print(paste("inner loop iteration:",j,"of",NUMBER))
            bestPrior = algorithm1(data1,10,PRIOR_RANGE,ALPHA_RANGE[i])
			if(bestPrior == 0){
				toPlot[nrow(toPlot),i] = toPlot[nrow(toPlot),i] + 1
			}
			if(bestPrior != 0){
				col = which.max(1*(PRIOR_RANGE == bestPrior))
				toPlot[nrow(toPlot) - col,i] = toPlot[nrow(toPlot) - col,i] + 1
			}
     }
  }


  # display heat map showing how often different priors are chosen
  color2D.matplot(toPlot,extremes=c('white','red'),xlab="alpha",ylab="Prior Parameter Chosen",axes=F,show.values=T,main="Frequency of Prior Parameter Chosen for Different Alpha")
  axis(1,at=(1:length(ALPHA_RANGE) - .5),labels=ALPHA_RANGE)
  axis(2,at=(c(0,1:length(PRIOR_RANGE)) + .5),labels=c(0,PRIOR_RANGE))

  # save the graphic
  pdf("loss_choices.pdf")
  color2D.matplot(toPlot,extremes=c('white','red'),xlab="alpha",ylab="Prior Parameter Chosen",axes=F,show.values=T,main="Frequency of Prior Parameter Chosen for Different Alpha")
  axis(1,at=(1:length(ALPHA_RANGE) - .5),labels=ALPHA_RANGE)
  axis(2,at=(c(0,1:length(PRIOR_RANGE)) + .5),labels=c(0,PRIOR_RANGE))
  dev.off()
}








###
### wrapper function that makes algorithm1, CV, alpha selection, ect work the way it should
###
### 1. ALPHA_RANGE is the proportion of bursts astronomers are able to follow up on
### 2. PRIORS is a vector of the choices for the priors
### 3. nCV # times we cross validate, high = stable est (must be > 1, this is a bug)
### 4. nCV1 is number folds used by function crossValidateAlgorithm, which CVs algorithm1
### 5. nCV2 is number of folds used by algorithm1
### 6. Z is actual redshift
implement = function(data1,Z=0,ALPHA_RANGE = c(.1,.3,.5),PRIORS = (1:9 / 10),nCV = 10,nCV1 = 10,nCV2 = 10,NUMBER_ALG1 = 1){
  if(length(Z) != nrow(data1)){
    Z = rep(0,nrow(data1))
  }
  confusionMatrices = array(0,c(2,2,nCV,length(ALPHA_RANGE))) # for storing results
  correct = matrix(0,nrow=length(Z),ncol=(length(ALPHA_RANGE)+1)) # record which redshift we predict correctly
  correct[,1] = Z # store actual redshift
  theFits = list() # record trees we create so we can see which features are split on 

  print("computing CV error estimates and others . . . part 2 of 2 of the program")
  for(i in 1:length(ALPHA_RANGE)){
    print(paste("outer loop iteration:",i,"of",length(ALPHA_RANGE)))
    theFits[[i]] = list()
    for(j in 1:nCV){
      print(paste("inner loop iteration:",j,"of",nCV))
      CVdata = crossValidateAlgorithm(data1,PRIORS,ALPHA_RANGE[i],nCV1,nCV2,NUMBER_ALG1,Z)
      confusionMatrices[,,j,i] = fillAlpha(CVdata[[1]],ALPHA_RANGE[i])
      theFits[[i]][[j]] = CVdata[[2]]
      correct[,i+1] = correct[,i+1] + CVdata[[3]]
    }
  }


### outputs the proportion of times classified correctly for GRBs with highest redshift
### this prints the data in correct.tex, which is nice for .tex. You can just print
### 'correct' variable to view in R.
  correct = correct[order(correct[,1],decreasing=TRUE),]
  correct = correct[1:30,] # only look at classification accuracy of 30 highest red shift GRBs
  correct[,2:ncol(correct)] = correct[,2:ncol(correct)] / nCV # number correct to proportion
  colnames(correct) = c("Redshift",ALPHA_RANGE)
  outputX = xtable(correct,caption="Fraction of Times Given Burst was Classified Correctly. This is before adjustment is made to fill alpha parameter so results are hard to interpret.",digits=2) # put into nice table
  print(outputX,type='latex',append=FALSE,file="correct.tex",table.placement="H",include.rownames=F) # print table


## this prints the variables that are most often used for splitting in a nice format for
## .tex. It is a bit harder to view this in R.
  filename1 = "splits.tex"
  for(i in 1:length(ALPHA_RANGE)){
    outputSplits(theFits[[i]],ALPHA_RANGE[i],filename1,i)
  }


## outputs confusion matrices in nice format, for viewing with .tex
  CONFUSION_NAME = 'confusions.tex'
  for(i in 1:length(ALPHA_RANGE)){
    prettyConfusion(confusionMatrices[,,,i],ALPHA_RANGE[i],i,CONFUSION_NAME)
  }
}
